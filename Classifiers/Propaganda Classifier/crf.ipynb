{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CRF import CRF\n",
    "from utils import crf_train_loop\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two dice one is fair, one is loaded\n",
    "fair_dice = np.array([1/6]*6)\n",
    "loaded_dice = np.array([0.04,0.04,0.04,0.04,0.04,0.8])\n",
    "\n",
    "probabilities = {'fair': fair_dice,\n",
    "                'loaded': loaded_dice}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dice is fair at time t, 0.6 chance we stay fair, 0.4 chance it is loaded at time 2\n",
    "transition_mat = {'fair': np.array([0.6, 0.4, 0.0]),\n",
    "                 'loaded': np.array([0.3, 0.7, 0.0]),\n",
    "                 'start': np.array([0.5, 0.5, 0.0])}\n",
    "states = ['fair', 'loaded', 'start']\n",
    "state2ix = {'fair': 0,\n",
    "           'loaded': 1,\n",
    "           'start': 2}\n",
    "\n",
    "log_likelihood = np.hstack([np.log(fair_dice).reshape(-1,1), \n",
    "                            np.log(loaded_dice).reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(n_timesteps):\n",
    "    data = np.zeros(n_timesteps)\n",
    "    prev_state = 'start'\n",
    "    state_list = np.zeros(n_timesteps)\n",
    "    for n in range(n_timesteps):\n",
    "        next_state = np.random.choice(states, p=transition_mat[prev_state])\n",
    "        state_list[n] = state2ix[next_state]\n",
    "        next_data = np.random.choice([0,1,2,3,4,5], p=probabilities[next_state])\n",
    "        data[n] = next_data\n",
    "        prev_state = next_state\n",
    "    return data, state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 15\n",
    "rolls = np.zeros((5000, n_obs)).astype(int)\n",
    "targets = np.zeros((5000, n_obs)).astype(int)\n",
    "\n",
    "for i in range(5000):\n",
    "    data, dices = simulate_data(n_obs)\n",
    "    rolls[i] = data.reshape(1, -1).astype(int)\n",
    "    targets[i] = dices.reshape(1, -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1438, -1.0999, -0.9955],\n",
      "        [-1.0071, -1.1127, -0.9919]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.init.normal_(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_train_loop(model, rolls, targets, n_epochs, learning_rate=0.01):\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate,\n",
    "                     weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_loss = []\n",
    "        N = rolls.shape[0]\n",
    "        model.zero_grad()\n",
    "        for index, (roll, labels) in enumerate(zip(rolls, targets)):\n",
    "            # Forward Pass\n",
    "            neg_log_likelihood = model.neg_log_likelihood(roll, labels)\n",
    "            batch_loss.append(neg_log_likelihood)\n",
    "            \n",
    "            if index % 50 == 0:\n",
    "                ll = torch.cat(batch_loss).mean()\n",
    "                ll.backward()\n",
    "                optimizer.step()\n",
    "                print(\"Epoch {}: Batch {}/{} loss is {:.4f}\".format(epoch, index//50,N//50,ll.data.numpy()))\n",
    "                batch_loss = []\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Inspired by http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dice, log_likelihood):\n",
    "        super(CRF, self).__init__()\n",
    "        \n",
    "        self.n_states = n_dice\n",
    "        self.transition = torch.nn.init.normal_(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1)\n",
    "        self.loglikelihood = log_likelihood\n",
    "    \n",
    "\n",
    "    def to_scalar(self, var):\n",
    "        return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "    def argmax(self, vec):\n",
    "        _, idx = torch.max(vec, 1)\n",
    "        return self.to_scalar(idx)\n",
    "        \n",
    "    # numerically stable log sum exp\n",
    "    # Source: http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "    def log_sum_exp(self, vec):\n",
    "        max_score = vec[0, self.argmax(vec)]\n",
    "        max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "        return max_score + \\\n",
    "               torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "    \n",
    "    def _data_to_likelihood(self, rolls):\n",
    "        \"\"\"Converts a numpy array of rolls (integers) to log-likelihood.\n",
    "        Input is one [1, n_rolls]\n",
    "        \"\"\"\n",
    "        return Variable(torch.FloatTensor(self.loglikelihood[rolls]), requires_grad=False)\n",
    "        \n",
    "    \n",
    "    def _compute_likelihood_numerator(self, loglikelihoods, states):\n",
    "        \"\"\"Computes numerator of likelihood function for a given sequence.\n",
    "        \n",
    "        We'll iterate over the sequence of states and compute the sum \n",
    "        of the relevant transition cost with the log likelihood of the observed\n",
    "        roll. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Matrix of n_obs x n_states. \n",
    "                            i,j entry is loglikelihood of observing roll i given state j\n",
    "            states: sequence of labels\n",
    "        Output:\n",
    "            score: torch Variable. Score of assignment. \n",
    "        \"\"\"\n",
    "        prev_state = self.n_states\n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        for index, state in enumerate(states):\n",
    "            score += self.transition[state, prev_state] + loglikelihoods[index, state]\n",
    "            prev_state = state\n",
    "        return score\n",
    "    \n",
    "    def _compute_likelihood_denominator(self, loglikelihoods):\n",
    "        \"\"\"Implements the forward pass of the forward-backward algorithm.\n",
    "        \n",
    "        We loop over all possible states efficiently using the recursive\n",
    "        relationship: alpha_t(j) = \\sum_i alpha_{t-1}(i) * L(x_t | y_t) * C(y_t | y{t-1} = i)\n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_numerator.\n",
    "                            This algorithm efficiently loops over all possible state sequences\n",
    "                            so no other imput is needed.\n",
    "        Output:\n",
    "            torch Variable. \n",
    "        \"\"\"\n",
    "\n",
    "        # Stores the current value of alpha at timestep t\n",
    "        prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            alpha_t = []\n",
    "\n",
    "            # Loop over all possible states\n",
    "            for next_state in range(self.n_states):\n",
    "                \n",
    "                # Compute all possible costs of transitioning to next_state\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +\\\n",
    "                                   roll[next_state].view(1, -1).expand(1, self.n_states)\n",
    "                \n",
    "#                 print(feature_function.shape)\n",
    "                alpha_t_next_state = prev_alpha + feature_function\n",
    "#                 print(alpha_t_next_state.shape)\n",
    "                x = self.log_sum_exp(alpha_t_next_state).view(1)\n",
    "#                 print('X: ')\n",
    "#                 print(x)\n",
    "#                 print(x.shape)\n",
    "                alpha_t.append(self.log_sum_exp(alpha_t_next_state).view(1))\n",
    "#                 print(len(alpha_t))\n",
    "            \n",
    "#             print(alpha_t)\n",
    "#             print(len(alpha_t))\n",
    "#             print('First:')\n",
    "#             print(alpha_t[0])\n",
    "#             print(alpha_t[0].shape)\n",
    "#             print('Second:')\n",
    "#             print(alpha_t[1])\n",
    "#             print(alpha_t[1].shape)\n",
    "            prev_alpha = torch.cat(alpha_t).view(1, -1)\n",
    "#         print(self.log_sum_exp(prev_alpha).view(1).shape)\n",
    "        return self.log_sum_exp(prev_alpha)\n",
    "    \n",
    "    def _viterbi_algorithm(self, loglikelihoods):\n",
    "        \"\"\"Implements Viterbi algorithm for finding most likely sequence of labels.\n",
    "        \n",
    "        Very similar to _compute_likelihood_denominator but now we take the maximum\n",
    "        over the previous states as opposed to the sum. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_denominator.\n",
    "        Output:\n",
    "            tuple. First entry is the most likely sequence of labels. Second is\n",
    "                   the loglikelihood of this sequence. \n",
    "        \"\"\"\n",
    "\n",
    "        argmaxes = []\n",
    "\n",
    "        # prev_delta will store the current score of the sequence for each state\n",
    "        prev_delta = self.transition[:, self.n_states].contiguous().view(1, -1) +\\\n",
    "                      loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            local_argmaxes = []\n",
    "            next_delta = []\n",
    "            for next_state in range(self.n_states):\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +\\\n",
    "                                   roll.view(1, -1) +\\\n",
    "                                   prev_delta\n",
    "                most_likely_state = self.argmax(feature_function)\n",
    "                score = feature_function[0][most_likely_state]\n",
    "                next_delta.append(score)\n",
    "                local_argmaxes.append(most_likely_state)\n",
    "            prev_delta = torch.cat(next_delta).view(1, -1)\n",
    "            argmaxes.append(local_argmaxes)\n",
    "        \n",
    "        final_state = self.argmax(prev_delta)\n",
    "        final_score = prev_delta[0][final_state]\n",
    "        path_list = [final_state]\n",
    "\n",
    "        # Backtrack through the argmaxes to find most likely state\n",
    "        for states in reversed(argmaxes):\n",
    "            final_state = states[final_state]\n",
    "            path_list.append(final_state)\n",
    "        \n",
    "        return np.array(path_list), final_score\n",
    "        \n",
    "    def neg_log_likelihood(self, rolls, states):\n",
    "        \"\"\"Compute neg log-likelihood for a given sequence.\n",
    "        \n",
    "        Input: \n",
    "            rolls: numpy array, dim [1, n_rolls]. Integer 0-5 showing value on dice.\n",
    "            states: numpy array, dim [1, n_rolls]. Integer 0, 1. 0 if dice is fair.\n",
    "        \"\"\"\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        states = torch.LongTensor(states)\n",
    "        \n",
    "        sequence_loglik = self._compute_likelihood_numerator(loglikelihoods, states)\n",
    "#         print('Numerator: ')\n",
    "#         print(sequence_loglik.shape)\n",
    "#         print('Numerator done')\n",
    "        denominator = self._compute_likelihood_denominator(loglikelihoods)\n",
    "        return denominator - sequence_loglik\n",
    "               \n",
    "    \n",
    "    def forward(self, rolls):\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        return self._viterbi_algorithm(loglikelihoods)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRF(2, log_likelihood)\n",
    "n_dice=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Batch 0/100 loss is 9.1227\n",
      "Epoch 0: Batch 1/100 loss is 6.8983\n",
      "Epoch 0: Batch 2/100 loss is 7.4736\n",
      "Epoch 0: Batch 3/100 loss is 7.2539\n",
      "Epoch 0: Batch 4/100 loss is 7.6379\n",
      "Epoch 0: Batch 5/100 loss is 6.9648\n",
      "Epoch 0: Batch 6/100 loss is 6.7741\n",
      "Epoch 0: Batch 7/100 loss is 7.0387\n",
      "Epoch 0: Batch 8/100 loss is 7.1012\n",
      "Epoch 0: Batch 9/100 loss is 6.5904\n",
      "Epoch 0: Batch 10/100 loss is 6.8525\n",
      "Epoch 0: Batch 11/100 loss is 6.6339\n",
      "Epoch 0: Batch 12/100 loss is 7.3245\n",
      "Epoch 0: Batch 13/100 loss is 6.8953\n",
      "Epoch 0: Batch 14/100 loss is 6.6914\n",
      "Epoch 0: Batch 15/100 loss is 7.1151\n",
      "Epoch 0: Batch 16/100 loss is 7.0650\n",
      "Epoch 0: Batch 17/100 loss is 6.7962\n",
      "Epoch 0: Batch 18/100 loss is 6.7170\n",
      "Epoch 0: Batch 19/100 loss is 7.1804\n",
      "Epoch 0: Batch 20/100 loss is 6.9839\n",
      "Epoch 0: Batch 21/100 loss is 7.2463\n",
      "Epoch 0: Batch 22/100 loss is 6.6751\n",
      "Epoch 0: Batch 23/100 loss is 6.9411\n",
      "Epoch 0: Batch 24/100 loss is 7.4206\n",
      "Epoch 0: Batch 25/100 loss is 6.4920\n",
      "Epoch 0: Batch 26/100 loss is 6.8778\n",
      "Epoch 0: Batch 27/100 loss is 7.0686\n",
      "Epoch 0: Batch 28/100 loss is 6.7121\n",
      "Epoch 0: Batch 29/100 loss is 7.3117\n",
      "Epoch 0: Batch 30/100 loss is 6.9788\n",
      "Epoch 0: Batch 31/100 loss is 6.6298\n",
      "Epoch 0: Batch 32/100 loss is 7.2458\n",
      "Epoch 0: Batch 33/100 loss is 7.1397\n",
      "Epoch 0: Batch 34/100 loss is 6.9675\n",
      "Epoch 0: Batch 35/100 loss is 7.2253\n",
      "Epoch 0: Batch 36/100 loss is 7.1890\n",
      "Epoch 0: Batch 37/100 loss is 7.3597\n",
      "Epoch 0: Batch 38/100 loss is 6.8026\n",
      "Epoch 0: Batch 39/100 loss is 6.9680\n",
      "Epoch 0: Batch 40/100 loss is 6.8856\n",
      "Epoch 0: Batch 41/100 loss is 7.1800\n",
      "Epoch 0: Batch 42/100 loss is 6.9927\n",
      "Epoch 0: Batch 43/100 loss is 6.8382\n",
      "Epoch 0: Batch 44/100 loss is 6.4840\n",
      "Epoch 0: Batch 45/100 loss is 6.5096\n",
      "Epoch 0: Batch 46/100 loss is 7.3516\n",
      "Epoch 0: Batch 47/100 loss is 7.3347\n",
      "Epoch 0: Batch 48/100 loss is 6.9153\n",
      "Epoch 0: Batch 49/100 loss is 7.2564\n",
      "Epoch 0: Batch 50/100 loss is 7.1598\n",
      "Epoch 0: Batch 51/100 loss is 7.0206\n",
      "Epoch 0: Batch 52/100 loss is 6.9985\n",
      "Epoch 0: Batch 53/100 loss is 6.9813\n",
      "Epoch 0: Batch 54/100 loss is 6.9046\n",
      "Epoch 0: Batch 55/100 loss is 7.1070\n",
      "Epoch 0: Batch 56/100 loss is 7.2610\n",
      "Epoch 0: Batch 57/100 loss is 6.9237\n",
      "Epoch 0: Batch 58/100 loss is 6.7680\n",
      "Epoch 0: Batch 59/100 loss is 6.4589\n",
      "Epoch 0: Batch 60/100 loss is 7.5029\n",
      "Epoch 0: Batch 61/100 loss is 7.0279\n",
      "Epoch 0: Batch 62/100 loss is 7.1061\n",
      "Epoch 0: Batch 63/100 loss is 7.0846\n",
      "Epoch 0: Batch 64/100 loss is 6.6635\n",
      "Epoch 0: Batch 65/100 loss is 6.4256\n",
      "Epoch 0: Batch 66/100 loss is 6.9532\n",
      "Epoch 0: Batch 67/100 loss is 6.4425\n",
      "Epoch 0: Batch 68/100 loss is 6.9448\n",
      "Epoch 0: Batch 69/100 loss is 6.9720\n",
      "Epoch 0: Batch 70/100 loss is 7.4628\n",
      "Epoch 0: Batch 71/100 loss is 6.8452\n",
      "Epoch 0: Batch 72/100 loss is 6.9019\n",
      "Epoch 0: Batch 73/100 loss is 6.8644\n",
      "Epoch 0: Batch 74/100 loss is 7.3260\n",
      "Epoch 0: Batch 75/100 loss is 7.4758\n",
      "Epoch 0: Batch 76/100 loss is 6.7352\n",
      "Epoch 0: Batch 77/100 loss is 6.9919\n",
      "Epoch 0: Batch 78/100 loss is 7.2918\n",
      "Epoch 0: Batch 79/100 loss is 7.2945\n",
      "Epoch 0: Batch 80/100 loss is 6.7280\n",
      "Epoch 0: Batch 81/100 loss is 6.7909\n",
      "Epoch 0: Batch 82/100 loss is 6.4332\n",
      "Epoch 0: Batch 83/100 loss is 7.0129\n",
      "Epoch 0: Batch 84/100 loss is 6.7709\n",
      "Epoch 0: Batch 85/100 loss is 7.2293\n",
      "Epoch 0: Batch 86/100 loss is 6.9751\n",
      "Epoch 0: Batch 87/100 loss is 6.4901\n",
      "Epoch 0: Batch 88/100 loss is 6.6490\n",
      "Epoch 0: Batch 89/100 loss is 6.9028\n",
      "Epoch 0: Batch 90/100 loss is 6.5804\n",
      "Epoch 0: Batch 91/100 loss is 7.0310\n",
      "Epoch 0: Batch 92/100 loss is 6.7276\n",
      "Epoch 0: Batch 93/100 loss is 6.1903\n",
      "Epoch 0: Batch 94/100 loss is 6.7099\n",
      "Epoch 0: Batch 95/100 loss is 6.9163\n",
      "Epoch 0: Batch 96/100 loss is 7.0269\n",
      "Epoch 0: Batch 97/100 loss is 6.5292\n",
      "Epoch 0: Batch 98/100 loss is 6.7978\n",
      "Epoch 0: Batch 99/100 loss is 6.7636\n"
     ]
    }
   ],
   "source": [
    "model = crf_train_loop(model, rolls, targets, 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./checkpoint.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./checkpoint.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dices = simulate_data(15)\n",
    "test_rolls = data.reshape(1, -1).astype(int)\n",
    "test_targets = dices.reshape(1, -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rolls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(test_rolls[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-04d9215891c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_targets' is not defined"
     ]
    }
   ],
   "source": [
    "test_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())[0].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, dices = simulate_data(15)\n",
    "test_rolls = data.reshape(1, -1).astype(int)\n",
    "test_targets = dices.reshape(1, -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rolls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(test_rolls[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
