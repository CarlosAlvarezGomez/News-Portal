{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "# Imports necessary libraries\n",
    "import torch\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import transformers\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from transformers import AdamW\n",
    "\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes paths to training, development, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates paths to data\n",
    "train_path = os.path.join(os.getcwd(), \"Data\", \"train\")\n",
    "dev_path = os.path.join(os.getcwd(), \"Data\", \"dev\")\n",
    "test_path = os.path.join(os.getcwd(), \"Data\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Tokenizer code was mostly made by Professor Claire Cardie from Cornell University\n",
    "\n",
    "Tokenizer function\n",
    "The following function read the files at the given paths and create two lists for each path. The first list contains a list of list of strings (one list of strings for each example), and the second one contains a list of list of ints (on list of ints for each example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(fname):\n",
    "    with open(fname, encoding='utf-8') as open_article:\n",
    "        lines = open_article.read()\n",
    "    return lines\n",
    "\n",
    "def read_labels(labels : str) -> List[Tuple[int, int]]:\n",
    "    \"processing of labels file\"\n",
    "    labels = labels.split(\"\\n\")[:-1]\n",
    "    labels = [tuple(map(int, l.split(\"\\t\")[-2:])) for l in labels]\n",
    "    return labels\n",
    "\n",
    "def sort_and_merge_labels(labels : List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n",
    "    \"sort labels, necessary for later splitting\"\n",
    "    if len(labels) == 0:\n",
    "        return labels\n",
    "    labels = list(sorted(labels, key = lambda t: t[0]))\n",
    "    # merge\n",
    "    curr = labels[0]\n",
    "    merged = []\n",
    "    for l in labels[1:]:\n",
    "        # if distinct, add\n",
    "        if l[0] > curr[1]:\n",
    "            merged.append(curr)\n",
    "            curr = l\n",
    "        # else merge\n",
    "        else:\n",
    "            curr = (curr[0], max(curr[1], l[1]))\n",
    "    merged.append(curr)\n",
    "    return merged\n",
    "\n",
    "def split_with_labels(labels : List[Tuple[int, int]], article : str) -> Tuple[List[str], List[int]]:\n",
    "    \"split text into segments based upon labels\"\n",
    "    if len(labels) == 0:\n",
    "        return [article], [0]\n",
    "    segments = []\n",
    "    binary_class = []\n",
    "    start = 0\n",
    "    for l_start, l_end in labels:\n",
    "        std_seg = article[start:l_start]\n",
    "        prop_seg = article[l_start:l_end]\n",
    "        segments.append(std_seg)\n",
    "        binary_class.append(0)\n",
    "        segments.append(prop_seg)\n",
    "        binary_class.append(1)\n",
    "        start = l_end\n",
    "    last_seg = article[start:]\n",
    "    segments.append(last_seg)\n",
    "    binary_class.append(0)\n",
    "    return segments, binary_class\n",
    "\n",
    "def remove_newline_fix_punc_seg(segments):\n",
    "    \"preprocessing necessary for tokenization to be consistent\"\n",
    "    segments = [s.replace(\"\\n\", \" \").replace(\".\", \" .\") for s in segments]\n",
    "    return segments\n",
    "\n",
    "def remove_newline_fix_punc_art(article):\n",
    "    \"preprocessing necessary for tokenization to be consistent\"\n",
    "    article = article.replace(\"\\n\", \" \").replace(\".\", \" .\")\n",
    "    return article\n",
    "\n",
    "def get_toks(input):\n",
    "    output = []\n",
    "    for toks in [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(input)]:\n",
    "        output += toks\n",
    "    return output\n",
    "\n",
    "# This is the function you may need to call\n",
    "def tokenize_article(article_file):\n",
    "    \"calls all functions above and perform sanity checks\"\n",
    "    article = read_txt(article_file)\n",
    "    article = remove_newline_fix_punc_art(article)\n",
    "    art_toks = get_toks(article)\n",
    "    return art_toks\n",
    "\n",
    "# This is the function you may need to call\n",
    "def master_tokenizer(article_file, labels_file):\n",
    "    \"calls all functions above and perform sanity checks\"\n",
    "    # read and get labels\n",
    "    article = read_txt(article_file)\n",
    "    labels = read_txt(labels_file)\n",
    "    labels = read_labels(labels)\n",
    "    labels = sort_and_merge_labels(labels)\n",
    "    segments, binary_class = split_with_labels(labels, article)\n",
    "    article = remove_newline_fix_punc_art(article)\n",
    "    segments = remove_newline_fix_punc_seg(segments)\n",
    "    # sanity check\n",
    "    reconstructed = \"\"\n",
    "    for seg, lab in zip(segments, binary_class):\n",
    "        reconstructed += seg\n",
    "    assert reconstructed == article\n",
    "    # tokenize\n",
    "    seg_toks = []\n",
    "    new_labels = []\n",
    "    for seg, label in zip(segments, binary_class):\n",
    "        new_toks = get_toks(seg)\n",
    "        seg_toks += new_toks\n",
    "        new_labels += [label for _ in range(len(new_toks))]\n",
    "        # sanity check\n",
    "    art_toks = get_toks(article)\n",
    "    sanity = True\n",
    "    if len(art_toks) != len(seg_toks):\n",
    "        sanity = False\n",
    "    for i, (at, st, lab) in enumerate(zip(art_toks, seg_toks, new_labels)):\n",
    "        if at != st:\n",
    "            sanity = False\n",
    "            break\n",
    "    return seg_toks, new_labels, sanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section uses the functions above to read the training and development data and save them as 4 variables: tokenized_train_articles, tokenized_train_labels, tokenized_dev_articles, tokenized_dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list -- file names of each article, sorted alphabetically\n",
    "train_articles_list = []\n",
    "dev_articles_list = []\n",
    "\n",
    "# list -- file names of each corresponding labels file, sorted alphabetically\n",
    "train_labels_list = []\n",
    "dev_labels_list = []\n",
    "\n",
    "for file_name in os.listdir(train_path):\n",
    "    if file_name[-11:] == '.labels.tsv':\n",
    "        train_labels_list.append(file_name)\n",
    "    elif file_name != 'test.task-SLC.labels' and file_name != '.article999001621.labels.tsv.swo':\n",
    "        train_articles_list.append(file_name)\n",
    "\n",
    "for file_name in os.listdir(dev_path):\n",
    "    if file_name[-11:] == '.labels.tsv':\n",
    "        dev_labels_list.append(file_name)\n",
    "    else:\n",
    "        dev_articles_list.append(file_name)\n",
    "\n",
    "train_articles_list = sorted(train_articles_list)\n",
    "train_labels_list = sorted(train_labels_list)\n",
    "dev_articles_list = sorted(dev_articles_list)\n",
    "dev_labels_list = sorted(dev_labels_list)\n",
    "\n",
    "# list of lists -- each list is a tokenized article\n",
    "tokenized_train_articles = []\n",
    "# list of lists -- each list is the labels corresponding to the article \n",
    "tokenized_train_labels = []\n",
    "\n",
    "# Creates a two lists: one containing each article as a list of strings, and one\n",
    "# containing the corresponding labels as a list of ints\n",
    "for i in range(len(train_articles_list)):\n",
    "    article_file = os.path.join(train_path, train_articles_list[i])\n",
    "    labels_file = os.path.join(train_path, train_labels_list[i])\n",
    "    art, lab, _ = master_tokenizer(article_file, labels_file)\n",
    "    tokenized_train_articles.append(art)\n",
    "    tokenized_train_labels.append(lab)\n",
    "\n",
    "# list of lists -- each list is a tokenized article\n",
    "tokenized_dev_articles = []\n",
    "# list of lists -- each list is the labels corresponding to the article \n",
    "tokenized_dev_labels = []\n",
    "\n",
    "# Creates a two lists: one containing each article as a list of strings, and one\n",
    "# containing the corresponding labels as a list of ints\n",
    "for i in range(len(dev_articles_list)):\n",
    "    article_file = os.path.join(dev_path, dev_articles_list[i])\n",
    "    labels_file = os.path.join(dev_path, dev_labels_list[i])\n",
    "    art, lab, _ = master_tokenizer(article_file, labels_file)\n",
    "    tokenized_dev_articles.append(art)\n",
    "    tokenized_dev_labels.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a set of binary tags and changes it to a set of BIO tags by changing every 1 that is not a leading 1 to a 2.\n",
    "def binary_tags_to_BIO(binary_labels):\n",
    "    for idx in range(1,len(binary_labels)):\n",
    "        if binary_labels[idx] == 1 and (binary_labels[idx-1] != 0):\n",
    "            binary_labels[idx] = 2\n",
    "    return binary_labels\n",
    "\n",
    "# Adds beginning of article and end of article labels\n",
    "def add_boa_and_eoa_tags(binary_labels, boa_label=3, eoa_label=4):\n",
    "    binary_labels.insert(0, boa_label)\n",
    "    binary_labels.append(eoa_label)\n",
    "    return binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_labels = list(map(add_boa_and_eoa_tags, list(map(binary_tags_to_BIO, tokenized_train_labels))))\n",
    "tokenized_dev_labels = list(map(add_boa_and_eoa_tags, list(map(binary_tags_to_BIO, tokenized_dev_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes trainloader and validation loader for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_to_idx(examples):\n",
    "    word_to_idx = {}\n",
    "    cur_idx = 1\n",
    "    for example in examples:\n",
    "        example_words = list(set(example))\n",
    "        for word in example_words:\n",
    "            if word not in word_to_idx.keys():\n",
    "                word_to_idx[word] = cur_idx\n",
    "                cur_idx += 1\n",
    "    word_to_idx['<unk>'] = cur_idx\n",
    "    return word_to_idx\n",
    "\n",
    "def vectorize_article(word_to_idx, lst, max_length):\n",
    "    result = torch.zeros(1,max_length)\n",
    "    vocab = word_to_idx.keys()\n",
    "    replace_unknowns = lambda word : word if word in vocab else '<unk>'\n",
    "    lst = list(map(replace_unknowns, lst))\n",
    "    for idx, word in enumerate(lst):\n",
    "        result[0,idx] = word_to_idx[word]\n",
    "    return result\n",
    "\n",
    "def vectorize_label(labels, max_length):\n",
    "    result = torch.zeros(1,max_length) - 1\n",
    "    for idx, word in enumerate(labels):\n",
    "        result[0,idx] = labels[idx]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def vectorize_example(word_to_idx, example, max_length):\n",
    "    article = example[0]\n",
    "    labels = example[1]\n",
    "    \n",
    "    return vectorize_article(word_to_idx, article, max_length), vectorize_label(labels, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8862])\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = get_word_to_idx(tokenized_train_articles)\n",
    "idx_to_word = {value:key for (key, value) in word_to_idx.items()}\n",
    "max_length = max(list(map(len, tokenized_train_labels)))\n",
    "\n",
    "vectorized_train_data = [vectorize_example(word_to_idx, ex, max_length) for ex in zip(tokenized_train_articles, tokenized_train_labels)]\n",
    "vectorized_dev_data = [vectorize_example(word_to_idx, ex, max_length) for ex in zip(tokenized_dev_articles, tokenized_dev_labels)]\n",
    "print(vectorized_train_data[5][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDataset(Dataset):\n",
    "    \"\"\"ModelDataset is a torch dataset to interact with the propoganda data.\n",
    "\n",
    "    :param data: The un-vectorized dataset with input and expected output values\n",
    "    :type data: List[Tuple[List[String], List[Int]]]\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.X = torch.cat([X for X, _ in data], dim=0)\n",
    "        self.Y = torch.cat([Y for _, Y in data], dim=0)\n",
    "        self.len = len(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"__len__ returns the number of samples in the dataset.\n",
    "\n",
    "        :returns: number of samples in dataset\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"__getitem__ returns the inputs, outputs pair for a given index\n",
    "\n",
    "        :param index: index within dataset to return\n",
    "        :type index: int\n",
    "        :returns: A tuple (x, y) where x is model inputs (pair of string-lists)\n",
    "                    and y is a bool and int-list pair\n",
    "        :rtype: Tuple[Tuple[List[String], List[String]], Tuple[Bool, List[int]]]\n",
    "        \"\"\"\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "def get_data_loaders(train, val, batch_size=8):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # First we create the dataset given our train and validation lists\n",
    "    dataset = ModelDataset(train+val)\n",
    "    \n",
    "    # Then, we create a list of indices for all samples in the dataset\n",
    "    train_indices = [i for i in range(len(train))]\n",
    "    val_indices = [i for i in range(len(train), len(train) + len(val))]\n",
    "    \n",
    "    # Now we define samplers and loaders for train and val\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    \n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_data_loaders(vectorized_train_data, vectorized_dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tunes a RoBERTa network using the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTa_CRF_Model_1(nn.Module):\n",
    "    def __init__(self, num_classes, tag_to_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\", add_prefix_space=True, model_max_length=512)\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-large')\n",
    "        self.encoding_to_probs = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        \n",
    "        self.transitions = nn.Parameter(torch.randn(self.num_classes, self.num_classes))\n",
    "\n",
    "        self.transitions.data[self.tag_to_idx['START_TAG'], :] = -10000\n",
    "        self.transitions.data[:, self.tag_to_idx['END_TAG']] = -10000\n",
    "        \n",
    "        \n",
    "    def _forward_alg(self, roberta_output):\n",
    "        \n",
    "        init_alphas = torch.full((1, self.num_classes), -10000.).to('cuda')\n",
    "        \n",
    "        init_alphas[0][self.tag_to_idx['START_TAG']] = 0.\n",
    "        \n",
    "        forward_var = init_alphas\n",
    "        \n",
    "         # Iterate through the sentence\n",
    "        for feat in roberta_output:\n",
    "            \n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            \n",
    "            for next_tag in range(self.num_classes):\n",
    "                \n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.num_classes)\n",
    "                \n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                \n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                \n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "                \n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "            \n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_idx['END_TAG']]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "        \n",
    "    def _get_roberta_probabilities(self, article):\n",
    "        tokenized_article = self.tokenizer(article, is_split_into_words=True, return_tensors='pt')\n",
    "        _ , article_length = tokenized_article['input_ids'].size()\n",
    "        input_ids = (tokenized_article['input_ids']).to('cuda')\n",
    "        if article_length > 512:\n",
    "            indices = torch.tensor([i for i in range(512)])\n",
    "            encodings = self.roberta(torch.index_select(input_ids, 1,indices.to('cuda'))).last_hidden_state\n",
    "            print(encodings.size())\n",
    "            n = 512\n",
    "            i=0\n",
    "            while n < article_length:\n",
    "                i += 1\n",
    "                print('Iteration: ' + str(i))\n",
    "                old_n = n\n",
    "                n = min(n+512, article_length)\n",
    "                print(old_n)\n",
    "                print(n)\n",
    "                indices = torch.tensor([i for i in range(old_n, n)]).to('cuda')\n",
    "                print(torch.index_select(input_ids, 1, indices).size())\n",
    "                cur_encodings = self.roberta(torch.index_select(input_ids, 1,indices)).last_hidden_state\n",
    "                print(cur_encodings.size())\n",
    "                print(encodings.size())\n",
    "                encodings = torch.cat((encodings, cur_encodings), 1)\n",
    "                print(encodings.size())\n",
    "        else:\n",
    "            encodings = self.roberta(**tokenized_article).last_hidden_state\n",
    "        \n",
    "        probabilities = self.encoding_to_probs(encodings)\n",
    "        probabilities = self.dropout(probabilities)\n",
    "        \n",
    "        return probabilities.reshape((article_length, self.num_classes))\n",
    "    \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_idx['START_TAG']], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_idx['END_TAG'], tags[-1]]\n",
    "        return score\n",
    "        \n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.num_classes), -10000.)\n",
    "        init_vvars[0][self.tag_to_idx['START_TAG']] = 0\n",
    "\n",
    "        forward_var = init_vvars\n",
    "        \n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.num_classes):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "                \n",
    "                \n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_idx['END_TAG']]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "            \n",
    "        start = best_path.pop()\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "        \n",
    "    def neg_log_likelihood(self, sentences, tag_sequences):\n",
    "        first = True\n",
    "        for (sentence, tags) in zip(sentences, tag_sequences):\n",
    "            probs = self._get_roberta_probabilities(sentence)\n",
    "            forward_score = self._forward_alg(probs)\n",
    "            gold_score = self._score_sentence(probs, tags)\n",
    "            if first:\n",
    "                loss = torch.sum(forward_score - gold_score)\n",
    "                first = False\n",
    "            else:\n",
    "                lost += torch.sum(forward_score - gold_score)\n",
    "        return loss\n",
    "        \n",
    "    def forward(self, articles):\n",
    "        \n",
    "        results = []\n",
    "        for article in articles:\n",
    "            roberta_probs = self._get_roberta_probabilities(article)\n",
    "\n",
    "            score, tag_seq = self._viterbi_decode(roberta_probs)\n",
    "            results.append((score, tag_seq))\n",
    "        return results\n",
    "    \n",
    "    def load_model(self, save_path):\n",
    "        self.load_state_dict(torch.load(save_path))\n",
    "        \n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_data, optimizer):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for (input_batch, expected_out) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        \n",
    "        model.zero_grad()\n",
    "        inputs = [\"\"]*len(input_batch)\n",
    "        outputs = [torch.empty(1)]*len(input_batch)\n",
    "        for idx, seq in enumerate(input_batch):\n",
    "            last_index = int((seq == 0).nonzero(as_tuple=True)[0][0])\n",
    "            inputs[idx] = \" \".join(list(map( lambda x : idx_to_word[x], seq[:last_index].tolist())))\n",
    "\n",
    "        \n",
    "        for idx, seq in enumerate(expected_out):\n",
    "            try:\n",
    "                last_index = int((seq == -1).nonzero(as_tuple=True)[0][0])\n",
    "            except:\n",
    "                print(last_index)\n",
    "            outputs[idx] = seq[:last_index]\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(inputs, outputs)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluation(model, val_loader, optimizer):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for (input_batch, expected_out) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
    "\n",
    "        sentence_in = prepare_sequence(input_batch, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "        \n",
    "        loss += model.neg_log_likelihood(sentence_in, expected_out)\n",
    "            \n",
    "    # Return loss\n",
    "    return loss\n",
    "\n",
    "def train_and_evaluate(number_of_epochs, model, train_loader, val_loader):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
    "        train_acc, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss = evaluation(model, val_loader, optimizer)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Epoch : \"+str(epoch+1)+\" | Train loss: \"+str(train_loss)+\" | Val loss: \"+str(val_loss))\n",
    "    return train_accuracies, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_idx = {'B': 1, 'I': 2, 'O': 0, 'START_TAG': 3, 'END_TAG': 4}\n",
    "model = RoBERTa_CRF_Model_1(5, tag_to_idx).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cc066a18c945cf89ebb564dda91b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs', max=3, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fa5be950fa4166833e4500f79202b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Batches', max=37, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4960 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 1024])\n",
      "Iteration: 1\n",
      "512\n",
      "1024\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 4.66 GiB already allocated; 14.44 MiB free; 4.68 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e714466e111f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-903f6f10629b>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(number_of_epochs, model, train_loader, val_loader)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mval_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Epochs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mtrain_accuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-903f6f10629b>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_data, optimizer)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Step 3. Run our forward pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-cb369e6f4960>\u001b[0m in \u001b[0;36mneg_log_likelihood\u001b[1;34m(self, sentences, tag_sequences)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_roberta_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mforward_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_alg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mgold_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-cb369e6f4960>\u001b[0m in \u001b[0;36m_get_roberta_probabilities\u001b[1;34m(self, article)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0mcur_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_encodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    512\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m                 )\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m         )\n\u001b[0;32m    401\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         )\n\u001b[0;32m    331\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relative_key\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relative_key_query\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 4.66 GiB already allocated; 14.44 MiB free; 4.68 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(3, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['Hello, my name is Bob', 'Hello Sammy, my name is Samuel']\n",
    "print(articles)\n",
    "print()\n",
    "probs = model._get_roberta_probabilities(articles[0])\n",
    "print(probs)\n",
    "prediction = model.forward(articles)\n",
    "print(prediction)\n",
    "# [0, 1437, 0, 2] '<s>'\n",
    "# [0, 1437, 2, 2] '</s>'\n",
    "# [0, 1437, 0, 1437, 2, 2] '<s> </s>'\n",
    "# [0, 1437, 0, 20920, 1437, 2, 2] '<s> Hello </s>'\n",
    "# [0, 1437, 0, 20920, 6, 127, 766, 16, 3045, 1437, 2, 2] '<s> Hello, my name is Bob </s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates functions that take in a sentence and returns a list of dictionaries, one for each token, containing features for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "glove25 = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary containing the features for the token at index in a given\n",
    "# article. This does not add part-of-speech tags because doing that on a token-\n",
    "# by-token basis was too innefficient. Instead, part-of-speech tags were added\n",
    "# in the next function\n",
    "def extractFeatureForToken(index, article):\n",
    "    global glove25\n",
    "    word = article[index]\n",
    "    embedding = glove25[word]\n",
    "    dic = {}\n",
    "    for i in range(len(embedding)):\n",
    "        dic[str(i)] = float(embedding[i])\n",
    "#     maxIndex = len(article)-1\n",
    "#     if (index-4 >= 0):\n",
    "#         prevWord4 = article[index-4]\n",
    "#     else:\n",
    "#         prevWord4 = \"\"\n",
    "\n",
    "#     if (index-3 >= 0):\n",
    "#         prevWord3 = article[index-3]\n",
    "#     else:\n",
    "#         prevWord3 = \"\"\n",
    "\n",
    "#     if (index-2 >= 0):\n",
    "#         prevWord2 = article[index-2]\n",
    "#     else:\n",
    "#         prevWord2 = \"\"\n",
    "\n",
    "#     if (index-1 >= 0):\n",
    "#         prevWord1 = article[index-1]\n",
    "#     else:\n",
    "#         prevWord1 = \"\"\n",
    "\n",
    "#     if (index+1 <= maxIndex):\n",
    "#         nextWord1 = article[index+1]\n",
    "#     else:\n",
    "#         nextWord1 = \"\"\n",
    "\n",
    "#     if (index+2 <= maxIndex):\n",
    "#         nextWord2 = article[index+2]\n",
    "#     else:\n",
    "#         nextWord2 = \"\"\n",
    "\n",
    "#     if (index+3 <= maxIndex):\n",
    "#         nextWord3 = article[index+3]\n",
    "#     else:\n",
    "#         nextWord3 = \"\"\n",
    "\n",
    "#     if (index+4 <= maxIndex):\n",
    "#         nextWord4 = article[index+4]\n",
    "#     else:\n",
    "#         nextWord4 = \"\"\n",
    "  \n",
    "#    dic = {\n",
    "#         \"prevWord4\" : prevWord4,\n",
    "#         \"prevWord3\" : prevWord3,\n",
    "#         \"prevWord2\" : prevWord2,\n",
    "#         \"prevWord1\" : prevWord1,\n",
    "#         \"word\" : article[index],\n",
    "#         \"nextWord1\" : nextWord1,\n",
    "#         \"nextWord2\" : nextWord2,\n",
    "#         \"nextWord3\" : nextWord3,\n",
    "#         \"nextWord4\" : nextWord4\n",
    "#     }\n",
    "    return dic\n",
    "\n",
    "# Extracts a feature dictionary for each token in an article and returns a list\n",
    "# of all dictionaries. \n",
    "def extractFeaturesForArticle(article):\n",
    "    indices = list(range(len(article)))\n",
    "    max_index = len(article)-1\n",
    "\n",
    "    pos = nltk.pos_tag(article)\n",
    "    featurelist = list(map(lambda x: extractFeatureForToken(x, article),indices))\n",
    "    for i in indices:\n",
    "#         phrase = \"\"\n",
    "#         if i-2 >=0:\n",
    "#             featurelist[i][\"prevWord2POS\"] = pos[i-2][1]\n",
    "#             phrase += (pos[i-2][1])\n",
    "#         else:\n",
    "#             featurelist[i][\"prevWord2POS\"] = \"\"\n",
    "#         if i-1 >=0:\n",
    "#             featurelist[i][\"prevWord1POS\"] = pos[i-1][1]\n",
    "#             phrase += (pos[i-1][1])\n",
    "#         else:\n",
    "#             featurelist[i][\"prevWord1POS\"] = \"\"\n",
    "#             phrase += (pos[i][1])\n",
    "#         if i+1 <= max_index:\n",
    "#             featurelist[i][\"nextWord1POS\"] = pos[i+1][1]\n",
    "#             phrase += (pos[i+1][1])\n",
    "#         else:\n",
    "#             featurelist[i][\"nextWord1POS\"] = \"\"\n",
    "#         if i+2 <= max_index:\n",
    "#             featurelist[i][\"nextWord2POS\"] = pos[i+2][1]\n",
    "#             phrase += (pos[i+2][1])\n",
    "#         else:\n",
    "#             featurelist[i][\"nextWord2POS\"] = \"\"\n",
    "#         featurelist[i][\"phrase\"] = phrase\n",
    "        featurelist[i][\"pos\"] = pos[i][1]\n",
    "    return featurelist\n",
    "\n",
    "# Extracts all feature dictionaries from a list of articles and returns a list\n",
    "# containing all dictionaries\n",
    "def extractFeaturesForAllArticles(articles, labels):\n",
    "\n",
    "    func = lambda x : extractFeaturesForArticle(articles[x])\n",
    "    listOfLists = list(map(func, list(range(len(articles)))))\n",
    "    finalList = []\n",
    "    for lst in listOfLists:\n",
    "        finalList += lst\n",
    "    return finalList\n",
    "\n",
    "# Flattens a list of lists into a single long list\n",
    "def flattenLabels(list_of_lists):\n",
    "    long_list = []\n",
    "    for lst in list_of_lists:\n",
    "        long_list += lst\n",
    "    return long_list\n",
    "\n",
    "# Takes in a list of articles and a list of list of labels and returns a list\n",
    "# of pair containing feature dictionaries and labels for each token\n",
    "def master_data_formatter(articles, labels):\n",
    "    feature_list  = extractFeaturesForAllArticles(articles, labels)\n",
    "    label_list = flattenLabels(labels)\n",
    "    return [(feature_list[i], label_list[i]) for i in range(len(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats the data using the helper function\n",
    "training_data = master_data_formatter(tokenized_train_articles, tokenized_train_labels)\n",
    "\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a maximum entropy classifier using the list of training feature vectors and the list of training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and trains the classifier\n",
    "classifier = MaxentClassifier.train(training_data, max_iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a Maximum Entropy Markov Model (MEMM) using the viterbi algorithm, viterbi_start function, viterbi_next function, and \n",
    "the classifier as the output function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "def memm_viterbi_output(token_idx, sent, state):\n",
    "    global features\n",
    "    global classifier\n",
    "    \n",
    "    return max(10**(-99),classifier.prob_classify(features[token_idx]).prob(state))\n",
    "\n",
    "def useMEMM(article):\n",
    "    global features\n",
    "    features = extractFeaturesForArticle(article)\n",
    "    return (viterbi(article, [0,1], viterbi_next, viterbi_start, memm_viterbi_output))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(map(useMEMM, tokenized_dev_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a list of lists of predicted labels and a list of lists of true\n",
    "# labels, and computed the micro f1 score of the predicitons\n",
    "def f1(predictions, truth):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for article_idx in range(len(predictions)):\n",
    "        predicted_labels = predictions[article_idx]\n",
    "        true_labels = truth[article_idx]\n",
    "        \n",
    "        for label_idx in range(len(predicted_labels)):\n",
    "            if (predicted_labels[label_idx] == 1) and (true_labels[label_idx] == 1):\n",
    "                tp += 1\n",
    "            elif (predicted_labels[label_idx] == 1):\n",
    "                fp += 1\n",
    "            elif (true_labels[label_idx] == 1):\n",
    "                fn += 1\n",
    "    \n",
    "    # Prints statement when about to divide by 0\n",
    "    if (tp + fp == 0):\n",
    "        print('Did not make any positive guesses')\n",
    "    elif (tp + fn == 0):\n",
    "        print('No postive values in the true labels')\n",
    "        \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1(predictions, tokenized_dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the average accuracry between the number of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Accuracy of viterbi: \" + str(weighted_accuracy(valid_labels, predictions)))\n",
    "\n",
    "#for k in range(100):\n",
    "#  classifier = MaxentClassifier.train(training_data, max_iter=k)\n",
    "#  predictions = list(map(useMEMM, valid_articles))\n",
    "#  print(\"Accuracy of viterbi after \" + str(k) + \" iterations: \" + str(weighted_accuracy(valid_labels, predictions)))\n",
    "\n",
    "# ERROR ANALYSIS: MEMM (ARTICLE 0)\n",
    "\n",
    "#print(\"Correct labels:\")\n",
    "#print(tokenized_dev_labels[1])\n",
    "#print(\"Output labels from Viterbi:\")\n",
    "#print(predictions[1])\n",
    "#count_correct_labels = 0\n",
    "#count_incorrect_labels = 0\n",
    "#incorrect_labels = []\n",
    "#for i in range(len(tokenized_dev_labels[0])):\n",
    "#    if tokenized_dev_labels[0][i] == predictions[0][i]:\n",
    "#        count_correct_labels += 1\n",
    "#    else:\n",
    "#        count_incorrect_labels += 1\n",
    "#        incorrect_labels.append(\"\\t\" + str(tokenized_dev_labels[0][i]) + \" was changed to \" + str(predictions[0][i]))\n",
    "\n",
    "#print(\"\\nNumber of correct labels: %d\" % count_correct_labels)\n",
    "#print(\"Number of incorrect labels: %d\" % count_incorrect_labels)\n",
    "#print(\"Incorrectly labelled:\")\n",
    "#for x in incorrect_labels:\n",
    "#    print(x)\n",
    "n = 0\n",
    "for prediction in predictions:\n",
    "    for label in prediction:\n",
    "        if label == 0:\n",
    "            n += 1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
